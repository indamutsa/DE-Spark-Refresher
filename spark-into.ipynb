{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/aindamutsa/anaconda3/envs/myenv/lib/python3.9/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/aindamutsa/anaconda3/envs/myenv/lib/python3.9/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Users/aindamutsa/anaconda3/envs/myenv/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/aindamutsa/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/aindamutsa/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/aindamutsa/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/aindamutsa/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/aindamutsa/anaconda3/envs/myenv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71ad81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bde1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Krish</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Indamutsa</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chantal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Armel</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Willy</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sandrine</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>41000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kevin</td>\n",
       "      <td>52.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Laura</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>33000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mohamed</td>\n",
       "      <td>31.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Krish</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Indamutsa</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Chantal</td>\n",
       "      <td>44.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Armel</td>\n",
       "      <td>33.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Willy</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>18000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sandrine</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>41000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kevin</td>\n",
       "      <td>52.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Laura</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jean</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>33000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>45.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mohamed</td>\n",
       "      <td>31.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ana√Øs</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Boris</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Fiona</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>36000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Gustavo</td>\n",
       "      <td>47.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name   Age  Experience   Salary  Unnamed: 4\n",
       "0       Krish  22.0         2.0  30000.0         NaN\n",
       "1   Indamutsa  34.0        10.0      NaN         NaN\n",
       "2     Chantal   NaN         8.0  20000.0         NaN\n",
       "3       Armel  33.0         NaN  15000.0         NaN\n",
       "4       Willy  34.0         3.0  18000.0         NaN\n",
       "5    Sandrine  23.0        18.0  41000.0         NaN\n",
       "6       Kevin  52.0        23.0      NaN         NaN\n",
       "7       Laura  29.0         6.0  22000.0         NaN\n",
       "8        Jean   NaN        14.0  33000.0         NaN\n",
       "9   Elizabeth  45.0         NaN  50000.0         NaN\n",
       "10    Mohamed  31.0         7.0      NaN         NaN\n",
       "11      Krish  22.0         2.0  30000.0         NaN\n",
       "12  Indamutsa  34.0        10.0  25000.0         NaN\n",
       "13    Chantal  44.0         8.0  20000.0         NaN\n",
       "14      Armel  33.0         5.0  15000.0         NaN\n",
       "15      Willy  34.0         3.0  18000.0         NaN\n",
       "16   Sandrine  23.0        18.0  41000.0         NaN\n",
       "17      Kevin  52.0        23.0      NaN         NaN\n",
       "18      Laura  29.0         6.0  22000.0         NaN\n",
       "19       Jean  39.0        14.0  33000.0         NaN\n",
       "20  Elizabeth  45.0        20.0  50000.0         NaN\n",
       "21    Mohamed  31.0         7.0  24000.0         NaN\n",
       "22      Ana√Øs  28.0         3.0  17000.0         NaN\n",
       "23      Boris  25.0         1.0  16000.0         NaN\n",
       "24      Fiona  38.0        12.0  36000.0         NaN\n",
       "25    Gustavo  47.0        22.0  55000.0         NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0f2c8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7cb16a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/24 19:39:48 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Practice \").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf04c745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv(\"test.csv\")\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "859d8088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_pyspark.show()\n",
    "df_pyspark = spark.read.option('header','true').csv('test.csv')\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb5c2215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(First Name=None, Department=None, Salary='52534.0', Years of Experience='5.0', Age='52.0'),\n",
       " Row(First Name='Olivia', Department='HR', Salary=None, Years of Experience='4.0', Age='51.0'),\n",
       " Row(First Name=None, Department='Sales', Salary=None, Years of Experience='11.0', Age=None)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ca8786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- Years of Experience: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5acd18",
   "metadata": {},
   "source": [
    "### Let us now Learn about:\n",
    "- PySpark DataFrames\n",
    "- Reading the DataSet\n",
    "- Checking the Datatypes of the columns (Schema)\n",
    "- Selecing Columns and Indexing\n",
    "- Check Describe option similar to Pandas\n",
    "- Adding Columns\n",
    "- Dropping Columns\n",
    "- Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0649f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the spark session\n",
    "spark = SparkSession.builder.appName('DataFrame').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852bb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df_pyspark = spark.read.option('header', 'true').csv('test.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7db937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way\n",
    "# df_pyspark = spark.read.csv('test.csv', header=True, inferSchema=True).show()\n",
    "df_pyspark = spark.read.csv('test.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42690a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()\n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Datatypes of the columns (Schema)\n",
    "\n",
    "# df_pyspark.columns\n",
    "# df_pyspark.head(3)\n",
    "# df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173873c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pyspark.select('Name').show()\n",
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb08037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select(['Name', 'Experience'])\n",
    "# df_pyspark.select(['Name', 'Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebc99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Describe option similar to Pandas\n",
    "\n",
    "df_pyspark.describe()\n",
    "# df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6929a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Columns in dataframe\n",
    "\n",
    "# df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience'] + 2)\n",
    "df_pyspark = df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience'] + 2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754fea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns\n",
    "# df_pyspark.drop('Experience After 2 Years').show()\n",
    "df_pyspark = df_pyspark.drop('Experience After 2 Years')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cf262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Columns\n",
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a1bb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1c4bf8e",
   "metadata": {},
   "source": [
    "# PySpark DataFrames\n",
    "- Dropping Columns\n",
    "- Dropping Rows\n",
    "- Various Parameter in Dropping functionalities\n",
    "- Handling Missing Data by Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856d9e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/24 15:11:25 WARN Utils: Your hostname, MacIndamutsa-2.local resolves to a loopback address: 127.0.0.1; using 192.168.0.12 instead (on interface en0)\n",
      "24/02/24 15:11:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/24 15:11:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf112040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  22|         2| 30000|\n",
      "|Indamutsa|  34|        10|  NULL|\n",
      "|  Chantal|NULL|         8| 20000|\n",
      "|    Armel|  33|      NULL| 15000|\n",
      "|    Willy|  34|         3| 18000|\n",
      "| Sandrine|  23|        18| 41000|\n",
      "|    Kevin|  52|        23|  NULL|\n",
      "|    Laura|  29|         6| 22000|\n",
      "|     Jean|NULL|        14| 33000|\n",
      "|Elizabeth|  45|      NULL| 50000|\n",
      "|  Mohamed|  31|         7|  NULL|\n",
      "|    Krish|  22|         2| 30000|\n",
      "|Indamutsa|  34|        10| 25000|\n",
      "|  Chantal|  44|         8| 20000|\n",
      "|    Armel|  33|         5| 15000|\n",
      "|    Willy|  34|         3| 18000|\n",
      "| Sandrine|  23|        18| 41000|\n",
      "|    Kevin|  52|        23|  NULL|\n",
      "|    Laura|  29|         6| 22000|\n",
      "|     Jean|  39|        14| 33000|\n",
      "+---------+----+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test.csv', header=True, inferSchema=True).drop('_c4')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e005e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the columns\n",
    "# df_pyspark.drop('Name').show()\n",
    "df_pyspark.na.drop().show() # Drop all rows where nan values are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### any == how\n",
    "### how == all\n",
    "df_pyspark.na.drop(how=\"any\").show()\n",
    "df_pyspark.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold\n",
    "df_pyspark.na.drop(how=\"any\", thresh=2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Threshold\n",
    "df_pyspark.na.drop(how=\"any\", subset=['Age']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1780843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  22|         2| 30000|\n",
      "|Indamutsa|  34|        10|     0|\n",
      "|  Chantal|NULL|         8| 20000|\n",
      "|    Armel|  33|         0| 15000|\n",
      "|    Willy|  34|         3| 18000|\n",
      "| Sandrine|  23|        18| 41000|\n",
      "|    Kevin|  52|        23|     0|\n",
      "|    Laura|  29|         6| 22000|\n",
      "|     Jean|NULL|        14| 33000|\n",
      "|Elizabeth|  45|         0| 50000|\n",
      "|  Mohamed|  31|         7|     0|\n",
      "|    Krish|  22|         2| 30000|\n",
      "|Indamutsa|  34|        10| 25000|\n",
      "|  Chantal|  44|         8| 20000|\n",
      "|    Armel|  33|         5| 15000|\n",
      "|    Willy|  34|         3| 18000|\n",
      "| Sandrine|  23|        18| 41000|\n",
      "|    Kevin|  52|        23|     0|\n",
      "|    Laura|  29|         6| 22000|\n",
      "|     Jean|  39|        14| 33000|\n",
      "+---------+----+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Filling the missing values\n",
    "# df_pyspark.na.fill(0).show()\n",
    "\n",
    "# wE can specify the columns you want to change\n",
    "# df_pyspark.na.fill(0, 'Experience').show()\n",
    "\n",
    "# We can specify all the columns that needs to be changed\n",
    "df_pyspark.na.fill(0, ['Experience', 'Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80fd3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now fill the missing values using median, mean or other summary statistics\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age', 'Experience','Salary']]\n",
    ").setStrategy(\"median\") # We can check also the medium, mode, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d2b0cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  22|         2| 30000|         22|                 2|         30000|\n",
      "|Indamutsa|  34|        10|  NULL|         34|                10|         24000|\n",
      "|  Chantal|NULL|         8| 20000|         33|                 8|         20000|\n",
      "|    Armel|  33|      NULL| 15000|         33|                 8|         15000|\n",
      "|    Willy|  34|         3| 18000|         34|                 3|         18000|\n",
      "| Sandrine|  23|        18| 41000|         23|                18|         41000|\n",
      "|    Kevin|  52|        23|  NULL|         52|                23|         24000|\n",
      "|    Laura|  29|         6| 22000|         29|                 6|         22000|\n",
      "|     Jean|NULL|        14| 33000|         33|                14|         33000|\n",
      "|Elizabeth|  45|      NULL| 50000|         45|                 8|         50000|\n",
      "|  Mohamed|  31|         7|  NULL|         31|                 7|         24000|\n",
      "|    Krish|  22|         2| 30000|         22|                 2|         30000|\n",
      "|Indamutsa|  34|        10| 25000|         34|                10|         25000|\n",
      "|  Chantal|  44|         8| 20000|         44|                 8|         20000|\n",
      "|    Armel|  33|         5| 15000|         33|                 5|         15000|\n",
      "|    Willy|  34|         3| 18000|         34|                 3|         18000|\n",
      "| Sandrine|  23|        18| 41000|         23|                18|         41000|\n",
      "|    Kevin|  52|        23|  NULL|         52|                23|         24000|\n",
      "|    Laura|  29|         6| 22000|         29|                 6|         22000|\n",
      "|     Jean|  39|        14| 33000|         39|                14|         33000|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to the dataframe\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74e487",
   "metadata": {},
   "source": [
    "## PySpark DataFrames\n",
    "- Filter Operations\n",
    "- &,|,==\n",
    "- ~ for not operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a601fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9196eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/24 15:32:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark  = SparkSession.builder.appName('dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc2e7ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  22|         2| 30000|\n",
      "|Indamutsa|  34|        10|  NULL|\n",
      "|  Chantal|NULL|         8| 20000|\n",
      "|    Armel|  33|      NULL| 15000|\n",
      "|    Willy|  34|         3| 18000|\n",
      "| Sandrine|  23|        18| 41000|\n",
      "|    Kevin|  52|        23|  NULL|\n",
      "|    Laura|  29|         6| 22000|\n",
      "|     Jean|NULL|        14| 33000|\n",
      "|Elizabeth|  45|      NULL| 50000|\n",
      "|  Mohamed|  31|         7|  NULL|\n",
      "|    Krish|  22|         2| 30000|\n",
      "|Indamutsa|  34|        10| 25000|\n",
      "|  Chantal|  44|         8| 20000|\n",
      "|    Armel|  33|         5| 15000|\n",
      "|    Willy|  34|         3| 18000|\n",
      "| Sandrine|  23|        18| 41000|\n",
      "|    Kevin|  52|        23|  NULL|\n",
      "|    Laura|  29|         6| 22000|\n",
      "|     Jean|  39|        14| 33000|\n",
      "+---------+----+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test.csv', header=True, inferSchema=True).drop('_c4')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bbb0a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Chantal|NULL|         8| 20000|\n",
      "|  Armel|  33|      NULL| 15000|\n",
      "|  Willy|  34|         3| 18000|\n",
      "|Chantal|  44|         8| 20000|\n",
      "|  Armel|  33|         5| 15000|\n",
      "|  Willy|  34|         3| 18000|\n",
      "|  Ana√Øs|  28|         3| 17000|\n",
      "|  Boris|  25|         1| 16000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Filter operations\n",
    "\n",
    "# Let us find the salary of people less than or equal to 20000\n",
    "df_pyspark.filter(\"Salary <= 20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04b99159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   Name| Age|\n",
      "+-------+----+\n",
      "|Chantal|NULL|\n",
      "|  Armel|  33|\n",
      "|  Willy|  34|\n",
      "|Chantal|  44|\n",
      "|  Armel|  33|\n",
      "|  Willy|  34|\n",
      "|  Ana√Øs|  28|\n",
      "|  Boris|  25|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us find the salary of people less than or equal to 20000\n",
    "df_pyspark.filter(\"Salary <= 20000\").select(['Name', 'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e53eaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Chantal|NULL|         8| 20000|\n",
      "|  Armel|  33|      NULL| 15000|\n",
      "|  Willy|  34|         3| 18000|\n",
      "|Chantal|  44|         8| 20000|\n",
      "|  Armel|  33|         5| 15000|\n",
      "|  Willy|  34|         3| 18000|\n",
      "|  Ana√Øs|  28|         3| 17000|\n",
      "|  Boris|  25|         1| 16000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Salary'] <= 20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5c095a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   Name| Age|Experience|Salary|\n",
      "+-------+----+----------+------+\n",
      "|Chantal|NULL|         8| 20000|\n",
      "|  Armel|  33|      NULL| 15000|\n",
      "|  Willy|  34|         3| 18000|\n",
      "|Chantal|  44|         8| 20000|\n",
      "|  Armel|  33|         5| 15000|\n",
      "|  Willy|  34|         3| 18000|\n",
      "|  Ana√Øs|  28|         3| 17000|\n",
      "|  Boris|  25|         1| 16000|\n",
      "+-------+----+----------+------+\n",
      "\n",
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  22|         2| 30000|\n",
      "|  Chantal|NULL|         8| 20000|\n",
      "|    Armel|  33|      NULL| 15000|\n",
      "|    Willy|  34|         3| 18000|\n",
      "| Sandrine|  23|        18| 41000|\n",
      "|    Laura|  29|         6| 22000|\n",
      "|     Jean|NULL|        14| 33000|\n",
      "|Elizabeth|  45|      NULL| 50000|\n",
      "|    Krish|  22|         2| 30000|\n",
      "|Indamutsa|  34|        10| 25000|\n",
      "|  Chantal|  44|         8| 20000|\n",
      "|    Armel|  33|         5| 15000|\n",
      "|    Willy|  34|         3| 18000|\n",
      "| Sandrine|  23|        18| 41000|\n",
      "|    Laura|  29|         6| 22000|\n",
      "|     Jean|  39|        14| 33000|\n",
      "|Elizabeth|  45|        20| 50000|\n",
      "|  Mohamed|  31|         7| 24000|\n",
      "|    Ana√Øs|  28|         3| 17000|\n",
      "|    Boris|  25|         1| 16000|\n",
      "+---------+----+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary'] <= 20000) & (df_pyspark['Salary'] >= 15000)).show()\n",
    "df_pyspark.filter((df_pyspark['Salary'] <= 20000) | (df_pyspark['Salary'] >= 15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78774020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| Age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  22|         2| 30000|\n",
      "| Sandrine|  23|        18| 41000|\n",
      "|    Laura|  29|         6| 22000|\n",
      "|     Jean|NULL|        14| 33000|\n",
      "|Elizabeth|  45|      NULL| 50000|\n",
      "|    Krish|  22|         2| 30000|\n",
      "|Indamutsa|  34|        10| 25000|\n",
      "| Sandrine|  23|        18| 41000|\n",
      "|    Laura|  29|         6| 22000|\n",
      "|     Jean|  39|        14| 33000|\n",
      "|Elizabeth|  45|        20| 50000|\n",
      "|  Mohamed|  31|         7| 24000|\n",
      "|    Fiona|  38|        12| 36000|\n",
      "|  Gustavo|  47|        22| 55000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(~(df_pyspark['Salary'] <= 20000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878af22",
   "metadata": {},
   "source": [
    "## Groupby and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f2c05d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+-------------------+---+------+---------+--------------------+\n",
      "|     Name|Department|Salary|Years_of_Experience|Age|Gender|Full_Time|               Email|\n",
      "+---------+----------+------+-------------------+---+------+---------+--------------------+\n",
      "|      Mia|        HR| 77505|                 12| 38|  Male|    false|    emma@example.com|\n",
      "|  Madison| Marketing|110771|                  3| 25|Female|    false|  olivia@example.com|\n",
      "|   Sophia|     Sales|103130|                 10| 42|  Male|     true|     ava@example.com|\n",
      "| Isabella| Marketing| 68541|                  1| 50|Female|    false|isabella@example.com|\n",
      "|   Harper|     Sales| 50922|                 11| 43|  Male|    false|  sophia@example.com|\n",
      "|    Emily|        HR| 98046|                  4| 41|  Male|     true|     mia@example.com|\n",
      "|     Emma|        IT| 72663|                  8| 35|  Male|     true|charlotte@example...|\n",
      "|   Amelia|        IT| 94510|                  7| 40|  Male|    false|  amelia@example.com|\n",
      "|  Abigail|        HR| 61821|                 11| 44|  Male|     true|  evelyn@example.com|\n",
      "| Victoria| Marketing| 68512|                  4| 52|Female|    false| abigail@example.com|\n",
      "|    Avery|        HR| 53862|                  2| 39|Female|     true|  harper@example.com|\n",
      "|Charlotte|        HR| 71856|                  9| 55|Female|    false|   emily@example.com|\n",
      "|    Sofia|     Sales| 57496|                  6| 32|Female|    false|elizabeth@example...|\n",
      "| Scarlett|   Finance|112321|                 11| 34|  Male|     true|   avery@example.com|\n",
      "|     Aria|   Finance| 95054|                 11| 52|  Male|     true|   sofia@example.com|\n",
      "|Elizabeth|     Sales|116997|                 15| 50|Female|     true|    ella@example.com|\n",
      "|   Evelyn| Marketing| 90404|                  1| 48|  Male|    false| madison@example.com|\n",
      "|      Ava|        IT|119438|                  1| 40|  Male|    false|scarlett@example.com|\n",
      "|     Ella|     Sales| 83346|                 11| 47|Female|     true|victoria@example.com|\n",
      "|   Olivia|     Sales| 51564|                 11| 53|Female|    false|    aria@example.com|\n",
      "+---------+----------+------+-------------------+---+------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp_data = spark.read.csv('employees_data_enhanced.csv', header=True, inferSchema=True)\n",
    "df_emp_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2a1f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Years_of_Experience: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Full_Time: boolean (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f91eea32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------------+--------+\n",
      "|     Name|sum(Salary)|sum(Years_of_Experience)|sum(Age)|\n",
      "+---------+-----------+------------------------+--------+\n",
      "| Isabella|      68541|                       1|      50|\n",
      "|      Ava|     119438|                       1|      40|\n",
      "| Victoria|      68512|                       4|      52|\n",
      "|     Ella|      83346|                      11|      47|\n",
      "|   Evelyn|      90404|                       1|      48|\n",
      "|  Madison|     110771|                       3|      25|\n",
      "|      Mia|      77505|                      12|      38|\n",
      "|    Emily|      98046|                       4|      41|\n",
      "|   Olivia|      51564|                      11|      53|\n",
      "|     Emma|      72663|                       8|      35|\n",
      "| Scarlett|     112321|                      11|      34|\n",
      "|Elizabeth|     116997|                      15|      50|\n",
      "|   Amelia|      94510|                       7|      40|\n",
      "|    Sofia|      57496|                       6|      32|\n",
      "|  Abigail|      61821|                      11|      44|\n",
      "|   Sophia|     103130|                      10|      42|\n",
      "|    Avery|      53862|                       2|      39|\n",
      "|     Aria|      95054|                      11|      52|\n",
      "|Charlotte|      71856|                       9|      55|\n",
      "|   Harper|      50922|                      11|      43|\n",
      "+---------+-----------+------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GroupBy\n",
    "# Grouped to find the maximum salary\n",
    "df_emp_data.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf3905d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------------------+--------+\n",
      "|Department|sum(Salary)|sum(Years_of_Experience)|sum(Age)|\n",
      "+----------+-----------+------------------------+--------+\n",
      "|     Sales|     463455|                      64|     267|\n",
      "|        HR|     363090|                      38|     217|\n",
      "|   Finance|     207375|                      22|      86|\n",
      "| Marketing|     338228|                       9|     175|\n",
      "|        IT|     286611|                      16|     115|\n",
      "+----------+-----------+------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by department to see who makes more money\n",
    "df_emp_data.groupBy('Department').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "431bc995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------------------+------------------+\n",
      "|Department|avg(Salary)|avg(Years_of_Experience)|          avg(Age)|\n",
      "+----------+-----------+------------------------+------------------+\n",
      "|     Sales|    77242.5|      10.666666666666666|              44.5|\n",
      "|        HR|    72618.0|                     7.6|              43.4|\n",
      "|   Finance|   103687.5|                    11.0|              43.0|\n",
      "| Marketing|    84557.0|                    2.25|             43.75|\n",
      "|        IT|    95537.0|       5.333333333333333|38.333333333333336|\n",
      "+----------+-----------+------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by department to see who average money\n",
    "df_emp_data.groupBy('Department').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e0e85b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|Department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    6|\n",
      "|        HR|    5|\n",
      "|   Finance|    2|\n",
      "| Marketing|    4|\n",
      "|        IT|    3|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by department to see how many departments\n",
    "df_emp_data.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "565b9676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|    1658759|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us aggregate to see the total expenditure\n",
    "df_emp_data.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3d91d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who is the max salary\n",
    "from pyspark.sql.functions import col, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "df5172cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+-------------------+---+------+---------+--------------------+\n",
      "|     Name|Department|Salary|Years_of_Experience|Age|Gender|Full_Time|               Email|\n",
      "+---------+----------+------+-------------------+---+------+---------+--------------------+\n",
      "|      Ava|        IT|119438|                  1| 40|  Male|    false|scarlett@example.com|\n",
      "|Elizabeth|     Sales|116997|                 15| 50|Female|     true|    ella@example.com|\n",
      "| Scarlett|   Finance|112321|                 11| 34|  Male|     true|   avery@example.com|\n",
      "|  Madison| Marketing|110771|                  3| 25|Female|    false|  olivia@example.com|\n",
      "|   Sophia|     Sales|103130|                 10| 42|  Male|     true|     ava@example.com|\n",
      "|    Emily|        HR| 98046|                  4| 41|  Male|     true|     mia@example.com|\n",
      "|     Aria|   Finance| 95054|                 11| 52|  Male|     true|   sofia@example.com|\n",
      "|   Amelia|        IT| 94510|                  7| 40|  Male|    false|  amelia@example.com|\n",
      "|   Evelyn| Marketing| 90404|                  1| 48|  Male|    false| madison@example.com|\n",
      "|     Ella|     Sales| 83346|                 11| 47|Female|     true|victoria@example.com|\n",
      "|      Mia|        HR| 77505|                 12| 38|  Male|    false|    emma@example.com|\n",
      "|     Emma|        IT| 72663|                  8| 35|  Male|     true|charlotte@example...|\n",
      "|Charlotte|        HR| 71856|                  9| 55|Female|    false|   emily@example.com|\n",
      "| Isabella| Marketing| 68541|                  1| 50|Female|    false|isabella@example.com|\n",
      "| Victoria| Marketing| 68512|                  4| 52|Female|    false| abigail@example.com|\n",
      "|  Abigail|        HR| 61821|                 11| 44|  Male|     true|  evelyn@example.com|\n",
      "|    Sofia|     Sales| 57496|                  6| 32|Female|    false|elizabeth@example...|\n",
      "|    Avery|        HR| 53862|                  2| 39|Female|     true|  harper@example.com|\n",
      "|   Olivia|     Sales| 51564|                 11| 53|Female|    false|    aria@example.com|\n",
      "|   Harper|     Sales| 50922|                 11| 43|  Male|    false|  sophia@example.com|\n",
      "+---------+----------+------+-------------------+---+------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp_data.orderBy(col(\"Salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cfa66734",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TEMP_TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create the temporary view `employees` because it already exists.\nChoose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Using Spark SQL\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdf_emp_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memployees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/pyspark/sql/dataframe.py:370\u001b[0m, in \u001b[0;36mDataFrame.createTempView\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a local temporary view with this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    The lifetime of this temporary table is tied to the :class:`SparkSession`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m \n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TEMP_TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create the temporary view `employees` because it already exists.\nChoose a different name, drop or replace the existing view,  or add the IF NOT EXISTS clause to tolerate pre-existing views."
     ]
    }
   ],
   "source": [
    "# Using Spark SQL\n",
    "\n",
    "df_emp_data.createTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95de6421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------------------+---+\n",
      "|     Name|Salary|Years_of_Experience|Age|\n",
      "+---------+------+-------------------+---+\n",
      "|      Ava|119438|                  1| 40|\n",
      "|Elizabeth|116997|                 15| 50|\n",
      "| Scarlett|112321|                 11| 34|\n",
      "|  Madison|110771|                  3| 25|\n",
      "|   Sophia|103130|                 10| 42|\n",
      "|    Emily| 98046|                  4| 41|\n",
      "|     Aria| 95054|                 11| 52|\n",
      "|   Amelia| 94510|                  7| 40|\n",
      "|   Evelyn| 90404|                  1| 48|\n",
      "|     Ella| 83346|                 11| 47|\n",
      "|      Mia| 77505|                 12| 38|\n",
      "|     Emma| 72663|                  8| 35|\n",
      "|Charlotte| 71856|                  9| 55|\n",
      "| Isabella| 68541|                  1| 50|\n",
      "| Victoria| 68512|                  4| 52|\n",
      "|  Abigail| 61821|                 11| 44|\n",
      "|    Sofia| 57496|                  6| 32|\n",
      "|    Avery| 53862|                  2| 39|\n",
      "|   Olivia| 51564|                 11| 53|\n",
      "|   Harper| 50922|                 11| 43|\n",
      "+---------+------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT Name, Salary, Years_of_Experience, Age\n",
    "    FROM employees\n",
    "    ORDER BY Salary DESC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8445ba16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------------+-------+\n",
      "|Department|max_Salary|max_Years_of_Experience|max_Age|\n",
      "+----------+----------+-----------------------+-------+\n",
      "|        IT|    119438|                      8|     40|\n",
      "|     Sales|    116997|                     15|     53|\n",
      "|   Finance|    112321|                     11|     52|\n",
      "| Marketing|    110771|                      4|     52|\n",
      "|        HR|     98046|                     12|     55|\n",
      "+----------+----------+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also group by and make department and find the max for each columns\n",
    "df_grouped = df_emp_data.groupBy(\"Department\").agg(\n",
    "    max(\"Salary\").alias(\"max_Salary\"),\n",
    "    max(\"Years_of_Experience\").alias(\"max_Years_of_Experience\"),\n",
    "    max(\"Age\").alias(\"max_Age\")\n",
    ")\n",
    "\n",
    "\n",
    "df_sorted = df_grouped.orderBy(col(\"max_salary\").desc())\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "874eb1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------------------+--------+\n",
      "|     Name|avg(Salary)|avg(Years_of_Experience)|avg(Age)|\n",
      "+---------+-----------+------------------------+--------+\n",
      "| Isabella|    68541.0|                     1.0|    50.0|\n",
      "|      Ava|   119438.0|                     1.0|    40.0|\n",
      "| Victoria|    68512.0|                     4.0|    52.0|\n",
      "|     Ella|    83346.0|                    11.0|    47.0|\n",
      "|   Evelyn|    90404.0|                     1.0|    48.0|\n",
      "|  Madison|   110771.0|                     3.0|    25.0|\n",
      "|      Mia|    77505.0|                    12.0|    38.0|\n",
      "|    Emily|    98046.0|                     4.0|    41.0|\n",
      "|   Olivia|    51564.0|                    11.0|    53.0|\n",
      "|     Emma|    72663.0|                     8.0|    35.0|\n",
      "| Scarlett|   112321.0|                    11.0|    34.0|\n",
      "|Elizabeth|   116997.0|                    15.0|    50.0|\n",
      "|   Amelia|    94510.0|                     7.0|    40.0|\n",
      "|    Sofia|    57496.0|                     6.0|    32.0|\n",
      "|  Abigail|    61821.0|                    11.0|    44.0|\n",
      "|   Sophia|   103130.0|                    10.0|    42.0|\n",
      "|    Avery|    53862.0|                     2.0|    39.0|\n",
      "|     Aria|    95054.0|                    11.0|    52.0|\n",
      "|Charlotte|    71856.0|                     9.0|    55.0|\n",
      "|   Harper|    50922.0|                    11.0|    43.0|\n",
      "+---------+-----------+------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp_data.groupBy(\"Name\").avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071a0d4",
   "metadata": {},
   "source": [
    "## Exploring the MLlib Library of PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d517b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/24 17:19:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Practicing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b7b1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the dataset\n",
    "training = spark.read.csv(\"employees_data_enhanced.csv\", header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6f7b4398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+-------------------+---+------+---------+--------------------+\n",
      "|     Name|Department|Salary|Years_of_Experience|Age|Gender|Full_Time|               Email|\n",
      "+---------+----------+------+-------------------+---+------+---------+--------------------+\n",
      "|      Mia|        HR| 77505|                 12| 38|  Male|    false|    emma@example.com|\n",
      "|  Madison| Marketing|110771|                  3| 25|Female|    false|  olivia@example.com|\n",
      "|   Sophia|     Sales|103130|                 10| 42|  Male|     true|     ava@example.com|\n",
      "| Isabella| Marketing| 68541|                  1| 50|Female|    false|isabella@example.com|\n",
      "|   Harper|     Sales| 50922|                 11| 43|  Male|    false|  sophia@example.com|\n",
      "|    Emily|        HR| 98046|                  4| 41|  Male|     true|     mia@example.com|\n",
      "|     Emma|        IT| 72663|                  8| 35|  Male|     true|charlotte@example...|\n",
      "|   Amelia|        IT| 94510|                  7| 40|  Male|    false|  amelia@example.com|\n",
      "|  Abigail|        HR| 61821|                 11| 44|  Male|     true|  evelyn@example.com|\n",
      "| Victoria| Marketing| 68512|                  4| 52|Female|    false| abigail@example.com|\n",
      "|    Avery|        HR| 53862|                  2| 39|Female|     true|  harper@example.com|\n",
      "|Charlotte|        HR| 71856|                  9| 55|Female|    false|   emily@example.com|\n",
      "|    Sofia|     Sales| 57496|                  6| 32|Female|    false|elizabeth@example...|\n",
      "| Scarlett|   Finance|112321|                 11| 34|  Male|     true|   avery@example.com|\n",
      "|     Aria|   Finance| 95054|                 11| 52|  Male|     true|   sofia@example.com|\n",
      "|Elizabeth|     Sales|116997|                 15| 50|Female|     true|    ella@example.com|\n",
      "|   Evelyn| Marketing| 90404|                  1| 48|  Male|    false| madison@example.com|\n",
      "|      Ava|        IT|119438|                  1| 40|  Male|    false|scarlett@example.com|\n",
      "|     Ella|     Sales| 83346|                 11| 47|Female|     true|victoria@example.com|\n",
      "|   Olivia|     Sales| 51564|                 11| 53|Female|    false|    aria@example.com|\n",
      "+---------+----------+------+-------------------+---+------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the training data\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "776c4dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Years_of_Experience: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Full_Time: boolean (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'Department',\n",
       " 'Salary',\n",
       " 'Years_of_Experience',\n",
       " 'Age',\n",
       " 'Gender',\n",
       " 'Full_Time',\n",
       " 'Email']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.printSchema()\n",
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "81052899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assembling the features\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureAssembler = VectorAssembler(inputCols=[\"Age\", \"Years_of_Experience\"], outputCol=\"Independent features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6c6dfa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------+-------------------+---+------+---------+--------------------+--------------------+\n",
      "|     Name|Department|Salary|Years_of_Experience|Age|Gender|Full_Time|               Email|Independent features|\n",
      "+---------+----------+------+-------------------+---+------+---------+--------------------+--------------------+\n",
      "|      Mia|        HR| 77505|                 12| 38|  Male|    false|    emma@example.com|         [38.0,12.0]|\n",
      "|  Madison| Marketing|110771|                  3| 25|Female|    false|  olivia@example.com|          [25.0,3.0]|\n",
      "|   Sophia|     Sales|103130|                 10| 42|  Male|     true|     ava@example.com|         [42.0,10.0]|\n",
      "| Isabella| Marketing| 68541|                  1| 50|Female|    false|isabella@example.com|          [50.0,1.0]|\n",
      "|   Harper|     Sales| 50922|                 11| 43|  Male|    false|  sophia@example.com|         [43.0,11.0]|\n",
      "|    Emily|        HR| 98046|                  4| 41|  Male|     true|     mia@example.com|          [41.0,4.0]|\n",
      "|     Emma|        IT| 72663|                  8| 35|  Male|     true|charlotte@example...|          [35.0,8.0]|\n",
      "|   Amelia|        IT| 94510|                  7| 40|  Male|    false|  amelia@example.com|          [40.0,7.0]|\n",
      "|  Abigail|        HR| 61821|                 11| 44|  Male|     true|  evelyn@example.com|         [44.0,11.0]|\n",
      "| Victoria| Marketing| 68512|                  4| 52|Female|    false| abigail@example.com|          [52.0,4.0]|\n",
      "|    Avery|        HR| 53862|                  2| 39|Female|     true|  harper@example.com|          [39.0,2.0]|\n",
      "|Charlotte|        HR| 71856|                  9| 55|Female|    false|   emily@example.com|          [55.0,9.0]|\n",
      "|    Sofia|     Sales| 57496|                  6| 32|Female|    false|elizabeth@example...|          [32.0,6.0]|\n",
      "| Scarlett|   Finance|112321|                 11| 34|  Male|     true|   avery@example.com|         [34.0,11.0]|\n",
      "|     Aria|   Finance| 95054|                 11| 52|  Male|     true|   sofia@example.com|         [52.0,11.0]|\n",
      "|Elizabeth|     Sales|116997|                 15| 50|Female|     true|    ella@example.com|         [50.0,15.0]|\n",
      "|   Evelyn| Marketing| 90404|                  1| 48|  Male|    false| madison@example.com|          [48.0,1.0]|\n",
      "|      Ava|        IT|119438|                  1| 40|  Male|    false|scarlett@example.com|          [40.0,1.0]|\n",
      "|     Ella|     Sales| 83346|                 11| 47|Female|     true|victoria@example.com|         [47.0,11.0]|\n",
      "|   Olivia|     Sales| 51564|                 11| 53|Female|    false|    aria@example.com|         [53.0,11.0]|\n",
      "+---------+----------+------+-------------------+---+------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = featureAssembler.transform(training)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "04e96142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'Department',\n",
       " 'Salary',\n",
       " 'Years_of_Experience',\n",
       " 'Age',\n",
       " 'Gender',\n",
       " 'Full_Time',\n",
       " 'Email',\n",
       " 'Independent features']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac6d6c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent features|Salary|\n",
      "+--------------------+------+\n",
      "|         [38.0,12.0]| 77505|\n",
      "|          [25.0,3.0]|110771|\n",
      "|         [42.0,10.0]|103130|\n",
      "|          [50.0,1.0]| 68541|\n",
      "|         [43.0,11.0]| 50922|\n",
      "|          [41.0,4.0]| 98046|\n",
      "|          [35.0,8.0]| 72663|\n",
      "|          [40.0,7.0]| 94510|\n",
      "|         [44.0,11.0]| 61821|\n",
      "|          [52.0,4.0]| 68512|\n",
      "|          [39.0,2.0]| 53862|\n",
      "|          [55.0,9.0]| 71856|\n",
      "|          [32.0,6.0]| 57496|\n",
      "|         [34.0,11.0]|112321|\n",
      "|         [52.0,11.0]| 95054|\n",
      "|         [50.0,15.0]|116997|\n",
      "|          [48.0,1.0]| 90404|\n",
      "|          [40.0,1.0]|119438|\n",
      "|         [47.0,11.0]| 83346|\n",
      "|         [53.0,11.0]| 51564|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data = output.select(\"Independent features\", \"Salary\")\n",
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b8d47d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/24 17:32:13 WARN Instrumentation: [8893a98e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/02/24 17:32:13 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/02/24 17:32:13 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# Training test split\n",
    "train_data, test_data = finalized_data.randomSplit([0.75,0.25])\n",
    "\n",
    "regressor=LinearRegression(featuresCol=\"Independent features\", labelCol='Salary')\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6eebc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-1459.567, 1295.6875])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "74d11c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139561.5255700838"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Intercepts\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "94bc67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47ea9700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|Independent features|Salary|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|          [32.0,6.0]| 57496|100629.50644167617|\n",
      "|          [35.0,8.0]| 72663| 98842.18038799867|\n",
      "|         [38.0,12.0]| 77505| 99646.22927638819|\n",
      "|          [40.0,7.0]| 94510| 90248.65792405751|\n",
      "|          [41.0,4.0]| 98046|  84902.0285123754|\n",
      "|         [42.0,10.0]|103130| 91216.58633999503|\n",
      "|         [44.0,11.0]| 61821| 89593.13981386552|\n",
      "|          [48.0,1.0]| 90404| 70797.99710920417|\n",
      "+--------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "22e25145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21018.848254287055, 565577805.5744939)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us check the error\n",
    "pred_results.meanAbsoluteError, pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "87095dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Let us save the model\n",
    "\n",
    "# Specifying the path where I save the model in DBFS\n",
    "model_path = \"./model_data\"\n",
    "\n",
    "# Save the model\n",
    "regressor.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679bb4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
